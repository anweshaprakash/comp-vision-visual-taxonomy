{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0586aa8-1170-4332-9bdf-4271ca05b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03109c-db4e-42d6-a7d3-7d2cf87098f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('categories.csv', index=False)\n",
    "\n",
    "print(\"Parquet file has been converted to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbd5f7-a22a-4ef7-8f26-eefb4db894d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_attributes = {\n",
    "    \"Men Tshirts\": [\"color\", \"neck\", \"pattern\", \"print_or_pattern_type\", \"sleeve_length\"],\n",
    "    \"Sarees\": [\"blouse_pattern\", \"border\", \"border_width\", \"color\", \"occasion\", \"ornamentation\", \"pallu_details\", \"pattern\", \"print_or_pattern_type\", \"transparency\"],\n",
    "    \"Kurtis\": [\"color\", \"fit_shape\", \"length\", \"occasion\", \"ornamentation\", \"pattern\", \"print_or_pattern_type\", \"sleeve_length\", \"sleeve_styling\"],\n",
    "    \"Women Tshirts\": [\"color\", \"fit_shape\", \"length\", \"pattern\", \"print_or_pattern_type\", \"sleeve_length\", \"sleeve_styling\", \"surface_styling\"],\n",
    "    \"Women Tops & Tunics\": [\"color\", \"fit_shape\", \"length\", \"neck_collar\", \"occasion\", \"pattern\", \"print_or_pattern_type\", \"sleeve_length\", \"sleeve_styling\", \"surface_styling\"]\n",
    "}\n",
    "\n",
    "train_df = pd.read_csv('train.meesho.csv')\n",
    "train_df.columns = train_df.columns.str.strip()\n",
    "\n",
    "attribute_classes = {attribute: [] for attribute in [\n",
    "    \"color\", \"neck\", \"pattern\", \"print_or_pattern_type\", \"sleeve_length\", \"blouse_pattern\", \n",
    "    \"border\", \"border_width\", \"occasion\", \"ornamentation\", \"pallu_details\", \"transparency\", \n",
    "    \"fit_shape\", \"length\", \"sleeve_styling\", \"surface_styling\", \"neck_collar\"]}\n",
    "\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    category = row['Category']\n",
    "    \n",
    "    if category in category_attributes:\n",
    "        attributes = category_attributes[category]\n",
    "       \n",
    "        for idx, attribute in enumerate(attributes):\n",
    "            attr_column = f'attr_{idx + 1}' \n",
    "        \n",
    "            if attr_column in train_df.columns:\n",
    "                attribute_value = row[attr_column]\n",
    "                \n",
    "                # Check if the value is not NaN before adding to the list\n",
    "                if pd.notna(attribute_value) and attribute_value not in attribute_classes[attribute]:\n",
    "                    attribute_classes[attribute].append(attribute_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7354233-67d9-4fdc-ad21-f30591cabe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df, category_attributes):\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Initialize a new DataFrame for storing encoded values\n",
    "    encoded = df.copy()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        category = row['Category']\n",
    "        \n",
    "        if category in category_attributes:\n",
    "            attributes = category_attributes[category]  # Get the attributes for this category\n",
    "            \n",
    "            for i, attr in enumerate(attributes):\n",
    "                attr_column = f'attr_{i+1}'\n",
    "                if attr_column in df.columns:\n",
    "                    if attr not in label_encoders:\n",
    "                        label_encoders[attr] = LabelEncoder()  # Initialize encoder for the attribute\n",
    "                        label_encoders[attr].fit(df[attr_column].astype(str))  # Fit on the data\n",
    "                    \n",
    "                    # Apply encoding to the respective column\n",
    "                    encoded[attr_column] = label_encoders[attr].transform(df[attr_column].astype(str))\n",
    "                else:\n",
    "                    print(f\"Warning: {attr_column} not found in the DataFrame for {category}\")\n",
    "\n",
    "    return encoded, label_encoders\n",
    "\n",
    "# Use the function to encode labels\n",
    "train_encoded, label_encoders = encode_labels(train_df, category_attributes)\n",
    "\n",
    "# Check the result\n",
    "print(train_encoded.head())\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d77f-580f-4a6f-b62b-fd1ac21daa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "base_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "outputs = [Dense(len(attribute_classes[attr]), activation='softmax', name=attr) for attr in attribute_classes]\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define generator for training data\n",
    "def image_generator(df, batch_size=32):\n",
    "    while True:\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            end = min(start + batch_size, len(df))\n",
    "            batch_df = df.iloc[start:end]\n",
    "            images = []\n",
    "            labels = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                image_id = str(row['id']).zfill(6) + '.jpg' \n",
    "                img = tf.keras.preprocessing.image.load_img(os.path.join(train_image_dir, image_id), target_size=(224, 224))\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                images.append(img)\n",
    "\n",
    "                # Prepare the labels\n",
    "                labels_batch = [train_labels[attr][i] for i, attr in enumerate(attribute_classes.keys())]\n",
    "                labels.append(labels_batch)\n",
    "\n",
    "            yield np.array(images), [np.array(label) for label in zip(*labels)]\n",
    "\n",
    "train_generator = image_generator(train_df, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94144c1f-73ed-4bf9-8da0-8c396dc2ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_generator, steps_per_epoch=len(train_df) // 32, epochs=10)\n",
    "\n",
    "# Predicting on test data\n",
    "def predict_on_test(df):\n",
    "    predictions = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = str(row['id']).zfill(6) + '.jpg'  # Correct image filename\n",
    "        img = tf.keras.preprocessing.image.load_img(os.path.join(test_image_dir, image_id), target_size=(224, 224))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        pred = model.predict(img)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "test_predictions = predict_on_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5e1b8-1a7b-43ee-9ba4-26e01804ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions and write to submission file\n",
    "def format_submission(predictions, df):\n",
    "    submission_data = []\n",
    "    for i, (pred, category) in enumerate(zip(predictions, df['Category'])):\n",
    "        attributes = []\n",
    "        for j, attr in enumerate(attribute_classes.keys()):\n",
    "            class_idx = np.argmax(pred[j]) \n",
    "            attribute_value = label_encoders[attr].inverse_transform([class_idx])[0]\n",
    "            attributes.append(attribute_value)\n",
    "        \n",
    "        \n",
    "        while len(attributes) < 10:\n",
    "            attributes.append('dummy_value')\n",
    "        \n",
    "        \n",
    "        row = [df['id'].iloc[i], category, len(attributes)] + attributes\n",
    "        submission_data.append(row)\n",
    "    \n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_data, columns=['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)])\n",
    "    submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "\n",
    "format_submission(test_predictions, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
